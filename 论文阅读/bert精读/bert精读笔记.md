# Abstract

1. 我们介绍一个新的语言表达模型——bert。

2. 不像其他的模型，bert被设计用于预训练深的双向的表示，使用的是没有标记的文本，再联合左右的上下文信息。

3. 由于设计的巧妙，我们可以只添加一个额外的输出层，就可以在很多的NLP任务上得到不错的结果，包括QA、语言推理，而且不需要针对任务做特别的架构上的改动。

> GPT考虑的是单向的，左边的上下文信息去预测未来。BERT使用的是双向的、左右的信息。
>
> ELMo是基于RNN的架构，BERT是transformer，ELMo在应用到下游任务时，需要对架构进行调整。



​	BERT模型在概念上更加简单，而且实验上更加好，它在11个NLP任务上得到了新的最好的结果。包括GLUE, SQuADV1.1等等。



# 1. Introduction

1. 语言模型中，预训练可以用来提升很多自然语言的任务。

2. 这些主要包含两类：第一类是句子层面的任务，例如自然语言推理和解释，其目标在于通过整体地分析句子来预测他们之间的关系。

3. 第二类是词语层面的任务，例如实体命名的识别和QA，这类任务需要细粒度（fine-grained）的词层面的输出。



1. 使用预训练模型做特征表征的时候一般有两类策略（strategy：基于特征的（feature-based）和基于微调的（fine-tuing）。

2. 基于特征的方法代表是ELMo，对于下游任务构造一个跟任务相关的神经网络（ELMo用的是RNN），将预训练后的表示作为额外的特征，和其他的输入一起输入到模型中。

3. 第二类是微调的：比如GPT，预训练好的模型放在下游任务的时候，不需要改变太多，只需要改一点点就可以了，模型预训练好的参数会在下游的数据赛再进行微调一下，就是说所有的权重根据新的数据进行微调。

4. 这两种途径是在预训练时候使用相同的目标函数，使用一个单向的语言模型去学习通用的语言表示



1. 现有的技术限制了预训练表达的能力，尤其是对于微调的方法。
2. 最大的局限在于，标准的语言模型是单向的，导致选择预训练的架构时收到了限制。
3. 例如，在GPT中，作者用了从左至右的架构，每个token在做self-attention的时候只能attend前面的内容。
4. 这样的限制在对于句子级的任务不是最优解，并且在一些基于微调的方法应用上是有问题的。例如QA，结合上下文是至关重要的。



1. 这篇文章里面，我们提出了bert来改进了微调的方法
2. bert减轻了之前提出的单向的限制，通过使用masked language model MLM预训练目标，这是收到了Cloze task的启发。
3. MLM随机的盖住了输入的一些词汇，目标是仅通过上下文预测被盖住的内容。
4. 不像从左到右的语言模型的预训练。MLM允许查看左右的上下文信息，这让我们可以预训练一个深的双向的transformer
5. 它还额外地训练了一个任务：下一个句子的预测。给你两个句子，判断他们在原文中是相邻的还是随机采样了两个句子。> 这样的任务让模型学到了句子层面的信息。文章的三点贡献：

1. 展现了双向信息的重要性。不像单向的。之前有个工作的将从左到右和从右到左的模型何简单地合在一起，bert在双向信息的应用上更好
2. 不需要对模型有大的改动，bert是第一个基于微调的模型，在句子层面和词语层面的任务上都取得了很好的成绩
3. code和模型开源



# 2. related work

# 2.1 非监督的基于特征的方法

ELMo、词嵌入

## 2.2 非监督的基于微调的方法

GPT

## 2.3 在有标记的数据上做迁移学习

CV运用广泛，NLP用这种方法不是很理想。



# 3.bert

1. 介绍bert的实现细节

2. 分两个步骤：预训练和微调
3. 在预训练里，模型是在没有标记的数据上训练。
4. 在微调时，同样使用了一个bert模型，但是它的参数被初始化为预训练后得到的参数，然后所有的参数都会参与训练，使用的是带标记的数据。
5. 每一个下游任务都会创建一个新的bert模型，初始化都用的是预训练的参数，但是下游任务会根据自己的有标记的数据调整参数。



### 模型架构

1.bert模型是一个多层的双向的transformer的encoder。



1.调了三个参数：

①transformer块的个数L 

②隐藏层的大小H

③self-attention头的个数A



2.有两个模型：

BERTBASE(L=12, H=768, A=12, Total Parameters=110M) 

和BERTLARGE(L=24, H=1024,A=16, Total Parameters=340M)



1.BERTBASE参数数量GPT相接近



### 输入和输出

1. bert的输入可以是一个句子或一个句子对。
2. 一个句子的意思是一段连续的文字，而不是真正语义上的句子。
3. 序列指的是bert的输入token sequence，可以是多个句子组合在一起



1. 采用了WordPiece的切词方法，大概有3万个词汇

   > 这个方法是切词根

2. 序列的第一个词永远是特殊标记**[CLS]**,代表classification

3. 这个token在最后的隐藏层中输出代表整个序列的信息。

4. 句子对被合在了一切作为一个sequence

5. 通过两种办法区分这两个句子

6. 一个是用token **[SEP]**分割

7. 另一种是添加一个embedding来表示这个句子是第一个句子还是第二个句子。



对于一个token，它进入bert后的embedding表示有三部分：token本身的embedding，区分在哪一个句子的embedding，位置的embedding，如图示

![](.\Snipaste_2022-01-17_21-22-19.jpg)





## 3.1 bert 预训练

### task1 Masked LM

1. 直观地说，我们有理由相信，深度双向模型严格来说比从左到右模型或从左到右和从右到左模型的浅层串联更强大。
2. 不幸的是，标准的条件语言模型只能从左到右、从右到左进行训练，因为双向条件会让每个单词间接地“看到自己”，而模型可以在多层语境中简单地预测目标单词



1. 为了训练深度双向表示，我们只是随机地屏蔽了某些百分比的输入token，然后预测这些屏蔽的token。

2. 我们将这个过程称为“蒙面LM”(MLM)，尽管在文献中它经常被称为cloztask (Taylor,1953)。

3. 在这种情况下，与掩码标记相对应的最终隐藏向量被输入到词汇表上的输出softmax中，就像在标准LM中一样。

4. 在我们所有的实验中，我们随机屏蔽了每个序列中15%的WordPiece标记。

   > 不替换特殊词元，CLS和SEP。对于1000长的句子，我们要预测150个词。

5. 与去噪自动编码器相比(Vincent et al.，2008)，我们只预测被屏蔽的单词，而不是重建整个输入。



1. 尽管这允许我们获得一个双向的预训练模型，但缺点是我们正在创建预训练和微调之间的不匹配，因为[MASK]在微调期间不会出现。
2. 为了减轻这个问题，我们并不总是用实际的[MASK]来替换“被屏蔽”的单词。
3. 训练数据生成器随机选择token位置的15%进行预测
4. 对于被选中的这一个词，我们有百分之八十的概率用[MASK]来替换他，百分之10的概率用一个随机词来替换，百分之十的概率不做变动



### task2 下个句子预测

1. 许多下游任务eg QA和NLI需要基于理解句子之间的关系，这个能力不能直接从语言模型中获得
2. 为了训练一个理解句子关系的模型，我们进行预训练
3. 对于句子A和B，B有一半的概率在A后面，一半的概率不在。
4. 这样的预训练对于QA和NLI非常有用



### pre-training data

用了BooksCorpus和English Wikipedia



## 3.2 bert 微调

1. 微调是简单的，因为Transformer中的自注意机制允许BERT通过交换适当的输入和输出来对许多下游任务(无论它们涉及单个文本还是文本对)进行建模。
2. 对于包含了文本对的应用，一个长点的模式是在应用双向cross attention前，单独地编码文本对。
3. 而bert使用了self-attention机制去统一这两个阶段，因为使用self-attention去编码两个连接在一起的句子，有效地包含了两个句子之间的双向cross attention



1. 对于每个下游任务，我们设计任务的输入和输出。
2. 根据下游任务的要求，要么是拿到第一个词元[CLS]所对应的输出去做分类，或者是拿到所有词元对应的输出；拿到Bert的输出后再加一层任务相关的输出层，比如softmax获得最终的标号（分类）



1. 和预训练相比，微调的成本嫌贵较低



# 4. 实验

## 4.1 GLUE

GLUE包含了多种自然语言理解任务的合集。

把[CLS]对应的输出拿出来，和最终的隐藏层的输出做聚合。

学习一个分类的输出层，把他们放进softmax，就解决了一个多分类问题。



## 4.2 SQuAD

1. 斯坦福问答数据集(SQuAD v1.1)是一个由10万对众包的问题/答案的集合。

2. 给一个问题和一个包含了答案的维基百科的页面，任务是预测在页面里的答案片段的开始和结尾。
3. 学习S和E，和每个vector相乘，得到它对应的词元是开始或者结尾的概率。

## 4.4 SWAG

用于判断两个句子之间的关系



# 5.Ablation Studies

介绍Bert中的设计对于模型的贡献是怎样的。

## 5.2 模型大小对预训练的影响

NLP模型确实是越大效果越好。

5.3 假设不用Bert做微调的，而是把Bert的输出作为一个静态特征用在模型中会怎么？结论：效果确实没有微调那么好

# 6. 结论

最近一些实验表明，使用非监督的预训练效果是非常好的，使得资源不多（比如少样本）的任务也可以享用神经网络。主要工作是把前人的结果拓展到深的双向的架构上，使得同样的预训练模型能够处理大量的NLP语言任务。



